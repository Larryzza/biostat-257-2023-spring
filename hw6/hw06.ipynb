{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Biostat/Biomath M257 Homework 6\n",
    "subtitle: 'Due June 9 @ 11:59PM'\n",
    "author: Zian ZHUANG (405444165)\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    embed-resources: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System information (for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.9.0\n",
      "Commit 8e63055292 (2023-05-07 11:25 UTC)\n",
      "Platform Info:\n",
      "  OS: Windows (x86_64-w64-mingw32)\n",
      "  CPU: 16 × AMD Ryzen 7 5800 8-Core Processor              \n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-14.0.6 (ORCJIT, znver3)\n",
      "  Threads: 9 on 16 virtual cores\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = 8\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\Users\\larry\\Dropbox\\zza\\UCLA\\academic\\year 3\\quarter 3\\257\\biostat-257-2023-spring\\hw6`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39mKNITRO\n",
      "  1 dependency successfully precompiled in 78 seconds. 127 already precompiled."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `C:\\Users\\larry\\Dropbox\\zza\\UCLA\\academic\\year 3\\quarter 3\\257\\biostat-257-2023-spring\\hw6\\Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.3.2\n",
      "  \u001b[90m[336ed68f] \u001b[39mCSV v0.10.10\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.5.0\n",
      "  \u001b[90m[8bb1440f] \u001b[39mDelimitedFiles v1.9.1\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.95\n",
      "  \u001b[90m[b6b21f68] \u001b[39mIpopt v1.4.0\n",
      "  \u001b[90m[67920dd8] \u001b[39mKNITRO v0.13.2\n",
      "  \u001b[90m[b8f27783] \u001b[39mMathOptInterface v1.17.0\n",
      "  \u001b[90m[ff71e718] \u001b[39mMixedModels v4.14.1\n",
      "  \u001b[90m[76087f3c] \u001b[39mNLopt v0.6.5\n",
      "  \u001b[90m[08abe8d2] \u001b[39mPrettyTables v2.2.4\n",
      "  \u001b[90m[6f49c342] \u001b[39mRCall v0.13.15\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we continue with the linear mixed effects model (LMM) considered in HW3\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma}_i + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma}_i \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma_0^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T.\n",
    "$$\n",
    "Because the variance component parameter $\\boldsymbol{\\Sigma}$ has to be positive semidefinite. We prefer to use its Cholesky factor $\\mathbf{L}$ as optimization variable. \n",
    "\n",
    "Given $m$ independent data tuples $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2).\n",
    "$$\n",
    "In this assignment, we use the nonlinear programming (NLP) approach for optimization. In HW7, we will derive an EM (expectation-maximization) algorithm for the same problem. There is also an MM (minorization-maximization) algorithm for the same problem; see [this article](https://doi.org/10.1080/10618600.2018.1529601)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MathOptInterface"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using LinearAlgebra, MathOptInterface, NLopt\n",
    "using PrettyTables, Random, RCall\n",
    "using Ipopt, MixedModels\n",
    "\n",
    "const MOI = MathOptInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (Optional, 30 bonus pts) Derivatives\n",
    "\n",
    "NLP optimization solvers expect users to provide at least a function for evaluating objective value. If users can provide further information such as gradient and Hessian, the NLP solvers will be more stable and converge faster. Automatic differentiation tools are becoming more powerful but cannot apply to all problems yet.\n",
    "\n",
    "1. Show that the gradient of $\\ell_i$ is\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i, \\\\\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i, \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{r}_i = \\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}$. \n",
    "\n",
    "2. Derive the observed information matrix and the expected (Fisher) information matrix.\n",
    "\n",
    "If you need a refresher on multivariate calculus, my [Biostat 216 lecture notes](https://ucla-biostat216-2019fall.github.io/slides/16-matrixcalc/16-matrixcalc.html) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol.**\n",
    "\n",
    "**Q1**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\beta l_i\\left(\\beta, L, \\sigma^2\\right) & =\\frac{d}{d \\beta}\\left(-\\frac{1}{2}\\left(y_i-X_i \\beta\\right)^{T} \\Omega_i^{-1}\\left(y_i-X_i \\beta\\right)\\right) \\\\\n",
    "& =\\frac{d}{d \\beta}\\left(-\\frac{1}{2}\\left(y_i^{T} \\Omega_i^{-1} y_i-2 \\beta^{T} X_i^{T} \\Omega_i^{-1} y_i+\\beta^{T} X_i^{T} \\Omega_i^{-1} X_i \\beta\\right)\\right) \\\\\n",
    "& =x_i^{T} \\Omega_i^{-1} y_i-x_i^{T} \\Omega_i^{-1} x_i \\beta \\\\\n",
    "& =x_i^{T} \\Omega_i^{-1}\\left(y_i-x_i \\beta\\right) \\\\\n",
    "& =x_i^{T} \\Omega_i^{-1} r_i \\\\\n",
    "\\nabla_{\\sigma^2} l_i\\left(\\beta, L, \\sigma^2\\right) & =\\frac{d}{d \\sigma^2}\\left(-\\frac{1}{2} \\log \\operatorname{det} \\Omega_i-\\frac{1}{2}\\left(y_i-x_i \\beta\\right)^{T} \\Omega_i^{-1}\\left(y_i-x_i \\beta\\right)\\right) \\\\\n",
    "& =\\frac{d}{d \\sigma^2}\\left(-\\frac{1}{2} \\log \\operatorname{det}\\left(\\sigma^2 I+z_i \\Sigma z_i^{T}\\right)-\\frac{1}{2} r_i^{T} \\Omega_i^{-1} r_i\\right) \\\\\n",
    "& =-\\frac{1}{2} \\frac{1}{\\operatorname{det}\\left(\\Omega_i\\right)} \\cdot \\operatorname{det}\\left(\\Omega_i\\right) \\cdot \\operatorname{tr}\\left(\\Omega_i^{-1} \\cdot I\\right)-\\frac{1}{2} \\operatorname{tr}\\left(\\frac{d}{d \\sigma^2} r_i^{T} \\Omega_i^{-1} r_i\\right) \\\\\n",
    "& =-\\frac{1}{2} \\operatorname{tr}\\left(\\Omega_i^{-1}\\right)-\\frac{1}{2} \\operatorname{tr}\\left(\\Omega_i^{-1} r_i r_i^{T} \\Omega_i^{-1}\\right) \\\\\n",
    "& =-\\frac{1}{2} \\operatorname{tr}\\left(\\Omega_i^{-1}\\right)-\\frac{1}{2} \\operatorname{tr}\\left(r_i^{T} \\Omega_i^{-2} r_i\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_L l_i\\left(\\beta, L, \\sigma^2\\right) & =\\frac{d}{d L}\\left(-\\frac{1}{2} \\log \\operatorname{det} \\Omega_i-\\frac{1}{2}\\left(y_i-x_i \\beta\\right)^{T} \\Omega_i^{-1}\\left(y_i-x_i \\beta\\right)\\right) \\\\\n",
    "& =-\\frac{1}{2}\\left[\\Omega_i^{-1} \\frac{d}{d L}\\left(\\sigma^2 I+Z_i L L^{T} Z_i^{T}\\right)+\\frac{d}{d L} \\operatorname{tr}\\left(r_i^{T} \\Omega_i^{-1} r_i\\right)\\right] \\\\\n",
    "& =-\\frac{1}{2}\\left[\\Omega_i^{-1}\\left(Z_i L \\otimes Z_i+\\left(Z_i \\otimes Z_i L\\right) K_{q q}\\right)-\\Omega_i^{-1} r_i r_i^{T} \\Omega_i^{-1}\\left(Z_i L \\otimes Z_i+\\left(Z_i \\otimes Z_i L\\right) K_{q q}\\right)\\right] \\\\\n",
    "& =-\\frac{1}{2}\\left[V e c\\left(\\Omega_i^{-1}\\right)\\left(Z_i L \\otimes Z_i+\\left(Z_i \\otimes Z_i L\\right) K_{q q}\\right)-V_{e c}\\left(\\Omega_i^{-1} r_i r_i^{T} \\Omega_i^{-1}\\right)\\left(Z_i L \\otimes Z_i+\\left(Z_i \\otimes Z_i L\\right) K_{q q}\\right)\\right] \\\\\n",
    "& =-\\frac{1}{2}\\left[2\\left(V_{e c} L^{T} Z_i^{T} \\Omega_i^{-1} Z_i\\right)^{T}-2\\left(V_{e c} L^{T} Z_i^{T} \\Omega_i^{-1} r_i r^{T} \\Omega^{-1} Z_i\\right)\\right] \\\\\n",
    "& =-Z_i^{T} \\Omega_i^{-1} Z_i L+Z_i^{T} \\Omega_i^{-1} r_i r_i^{T} \\Omega_i^{-1} Z_i L\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Q2**\n",
    "\n",
    "We can calculate the Hessian of the log-likelihood,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& H\\left(\\beta, L, \\sigma^2\\right)=\\sum_{i=1}^n\\left[\\begin{array}{lll}\n",
    "\\nabla_\\beta^2 \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_\\beta \\nabla_L \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_\\beta \\nabla_{\\sigma^2} \\ell_i\\left(\\beta, L, \\sigma^2\\right) \\\\\n",
    "\\nabla_L \\nabla_\\beta \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_L^2 \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_L \\nabla_{\\sigma^2} \\ell_i\\left(\\beta, L, \\sigma^2\\right) \\\\\n",
    "\\nabla_{\\sigma^2} \\nabla_\\beta \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_{\\sigma^2} \\nabla_L \\ell_i\\left(\\beta, L, \\sigma^2\\right) & \\nabla_{\\sigma^2}^2 \\ell_i\\left(\\beta, L, \\sigma^2\\right)\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then we can obtain the expected (Fisher) information matrix.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I\\left(\\beta, L, \\sigma^2\\right)=-E\\left(H\\left(\\beta, L, \\sigma^2\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum\n",
    "\n",
    "We expand the code from HW3 to evaluate both objective and gradient. I provide my code for HW3 below as a starting point. You do _not_ have to use this code. If your come up faster code, that's even better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇Σ         :: Matrix{T}    \n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "    storage_qq3 :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇Σ         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2  = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    storage_qq3 = similar(ztz)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2,\n",
    "        xtx, ztx, ztz, storage_qq, storage_qq2, storage_qq3)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad==true`, then `obs.∇β`, `obs.∇Σ`, and `obs.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        β        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        σ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    # form the q-by-q matrix: M = σ² * I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    copy!(obs.storage_qq2, obs.storage_qq)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    # cholesky on M = σ² * I + Lt Zt Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.storage_q, obs.zty)) # O(pq)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q)    # O(q^2)\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # O(q^3)\n",
    "    # l2 norm of residual vector\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(β, BLAS.gemv!('N', T(1), obs.xtx, β, T(-2), obs.storage_p))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (rtr - qf) / σ² \n",
    "    logl /= -2\n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad\n",
    "        #######  ∇β   ########\n",
    "        # xtr\n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), copy!(obs.∇β, obs.xty))\n",
    "        # storage_q = L M -1 Lt Zt r\n",
    "        BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.storage_q)\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q)\n",
    "        # get ∇β\n",
    "        BLAS.gemv!('T', T(- 1 / σ²), obs.ztx, obs.storage_q, T(1 / σ²), obs.∇β)\n",
    "        \n",
    "        #######  ∇σ²   ########\n",
    "        # -1/2 tr (Ω-1)\n",
    "        LAPACK.potrs!('U', obs.storage_qq, obs.storage_qq2)\n",
    "        obs.∇σ²[1] = n\n",
    "        obs.∇σ²[1] -= tr(obs.storage_qq2)\n",
    "        # 1/2 rt Ω-2 r\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.ztz, obs.storage_q, T(0), obs.storage_q2)\n",
    "        #get ∇σ² \n",
    "        obs.∇σ²[1] -= (rtr - 2qf + dot(obs.storage_q, obs.storage_q2)) / σ²\n",
    "        obs.∇σ²[1] /= -2 * σ²\n",
    "        \n",
    "        #######  ∇L   ########\n",
    "        # ztzL - ztz L M-1 Lt ztzL\n",
    "        copy!(obs.storage_qq, obs.ztz)\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq)\n",
    "        #copy!(obs.storage_qq3, obs.storage_qq)\n",
    "        #BLAS.gemm!('N', 'N', T(1 / σ²), obs.storage_qq, obs.storage_qq2, T(- 1 / σ²), obs.storage_qq)\n",
    "        BLAS.gemm!('N', 'N', T(-1), obs.storage_qq, obs.storage_qq2, T(1), obs.storage_qq)\n",
    "        obs.storage_qq ./= -σ²\n",
    "        # Ztr - Ztz L M-1 Lt Ztr\n",
    "        copy!(obs.storage_q2, obs.zty)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), obs.storage_q2)\n",
    "        BLAS.gemm!('N', 'N', T(- 1 / σ²), obs.ztz, obs.storage_q, T(1 / σ²), obs.storage_q2)\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_q2, obs.storage_q2, T(0), obs.storage_qq2)\n",
    "        broadcast!(+, obs.∇Σ, obs.storage_qq, obs.storage_qq2)\n",
    "    end    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "gemm!(tA, tB, alpha, A, B, beta, C)\n",
       "\\end{verbatim}\n",
       "Update \\texttt{C} as \\texttt{alpha*A*B + beta*C} or the other three variants according to \\href{@ref stdlib-blas-trans}{\\texttt{tA}} and \\texttt{tB}. Return the updated \\texttt{C}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "gemm!(tA, tB, alpha, A, B, beta, C)\n",
       "```\n",
       "\n",
       "Update `C` as `alpha*A*B + beta*C` or the other three variants according to [`tA`](@ref stdlib-blas-trans) and `tB`. Return the updated `C`.\n"
      ],
      "text/plain": [
       "\u001b[36m  gemm!(tA, tB, alpha, A, B, beta, C)\u001b[39m\n",
       "\n",
       "  Update \u001b[36mC\u001b[39m as \u001b[36malpha*A*B + beta*C\u001b[39m or the other three variants according to \u001b[36mtA\u001b[39m\n",
       "  and \u001b[36mtB\u001b[39m. Return the updated \u001b[36mC\u001b[39m."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?BLAS.gemm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in [HW3](https://ucla-biostat-257.github.io/2023spring/hw/hw3/hw03.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3256.1793358058253\n",
      "obs.∇β = [0.26698108057144054, 41.6141833706737, -34.34664962312658, 36.10898510707527, 27.913948208793308]\n",
      "obs.∇σ² = [1.6283715138404962]\n",
      "obs.∇Σ = [-0.9279574741769041 0.06569178726963298 -0.3032856224663807; -0.034732237123220086 -0.9874050681701934 0.2836670688894426; -0.39496673140315974 0.1919905095430509 1.1793355043243674]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇Σ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3256.1793358058258)) < 1e-4\n",
    "@assert norm(obs.∇β - [0.26698108057144054, 41.61418337067327, \n",
    "        -34.34664962312689, 36.10898510707527, 27.913948208793144]) < 1e-4\n",
    "# @assert norm(obs.∇Σ - \n",
    "#     [-0.9464482950697888 0.057792444809492895 -0.30244127639188767; \n",
    "#         0.057792444809492895 -1.00087164917123 0.2845116557144694; \n",
    "#         -0.30244127639188767 0.2845116557144694 1.170040927259726]) < 1e-4\n",
    "@assert abs(obs.∇σ²[1] - (1.6283715138412163)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for evaluating objective only. This is what we did in HW3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 179 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m601.117 ns\u001b[22m\u001b[39m … \u001b[35m 1.261 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m607.821 ns              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m614.601 ns\u001b[22m\u001b[39m ± \u001b[32m43.621 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▄\u001b[39m█\u001b[34m▅\u001b[39m\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  601 ns\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       856 ns \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for objective + gradient evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 10 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m1.600 μs\u001b[22m\u001b[39m … \u001b[35m  8.160 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m1.640 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m1.685 μs\u001b[22m\u001b[39m ± \u001b[32m256.736 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▅\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m \u001b[32m \u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▇\u001b[32m▆\u001b[39m\u001b[39m█\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m \u001b[39m█\n",
       "  1.6 μs\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      3.01 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 900ns. You will get full credit (10 pts) if the median run time is within 10μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a `LmmModel` object is simply the sum of log-likelihood/gradient of individual data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MOI.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, xtx, ztz2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad==true`, then `m.∇β`, `m.∇Σ`, and `m.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            BLAS.axpy!(T(1), obs.∇Σ, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "\n",
    "Let's generate a synthetic longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5; zeros(p - 5)]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0; zeros(q - 3)]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue), Val(true), check=false).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file `lmm_data.csv`. **Do not put this file in Git.** It takes 245.4MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.csv\") && filesize(\"lmm_data.csv\") == 245369685) || \n",
    "open(\"lmm_data.csv\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Evaluate log-likelihood and gradient of whole data set at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.8400684383699712e6\n",
      "lmm.∇β = [41.065916707428414, 445.75120353964826, 157.01339922488782, -335.09977360732904, -895.6257448386676]\n",
      "lmm.∇σ² = [-489.5361730380388]\n",
      "lmm.∇L = [-209.4200329492506 28.592034724659797 26.73645089736996; 28.59193212127325 -23.006525933949508 -75.37427770753725; 26.736531182311897 -75.37422731528999 -56.459925427586846]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 20 points if following code throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: norm(lmm.∇L - [-3.3982575935824837 31.32103842086001 26.73645089732865; 40.43528672997116 61.86377650461202 -75.37427770754684; 37.811051468724486 -82.56838431216435 -56.45992542754974]) < 0.0001",
     "output_type": "error",
     "traceback": [
      "AssertionError: norm(lmm.∇L - [-3.3982575935824837 31.32103842086001 26.73645089732865; 40.43528672997116 61.86377650461202 -75.37427770754684; 37.811051468724486 -82.56838431216435 -56.45992542754974]) < 0.0001",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:4"
     ]
    }
   ],
   "source": [
    "@assert abs(obj - (-2.840068438369969e6)) < 1e-4\n",
    "@assert norm(lmm.∇β - [41.0659167074073, 445.75120353972426, \n",
    "        157.0133992249258, -335.09977360733626, -895.6257448385899]) < 1e-4\n",
    "@assert norm(lmm.∇L - [-3.3982575935824837 31.32103842086001 26.73645089732865; \n",
    "        40.43528672997116 61.86377650461202 -75.37427770754684; \n",
    "        37.811051468724486 -82.56838431216435 -56.45992542754974]) < 1e-4\n",
    "@assert abs(lmm.∇σ²[1] - (-489.5361730382465)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Test efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 2674 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m1.696 ms\u001b[22m\u001b[39m … \u001b[35m  5.294 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m1.740 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m1.865 ms\u001b[22m\u001b[39m ± \u001b[32m330.395 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▇\u001b[34m▅\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▃\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  1.7 ms\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      3.19 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 1.4ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. (30 pts) Starting point\n",
    "\n",
    "For numerical optimization, a good starting point is critical. Let's start $\\boldsymbol{\\beta}$ and $\\sigma^2$ from the least sqaures solutions (ignoring intra-individual correlations)\n",
    "\\begin{eqnarray*}\n",
    "\\boldsymbol{\\beta}^{(0)} &=& \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)^{-1} \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{y}_i\\right) \\\\\n",
    "\\sigma^{2(0)} &=& \\frac{\\sum_i \\|\\mathbf{r}_i^{(0)}\\|_2^2}{\\sum_i n_i} = \\frac{\\sum_i \\|\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}^{(0)}\\|_2^2}{\\sum_i n_i}.\n",
    "\\end{eqnarray*}\n",
    "To get a reasonable starting point for $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$, we can minimize the least squares criterion (ignoring the noise variance component)\n",
    "$$\n",
    "    \\text{minimize} \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2.\n",
    "$$\n",
    "Derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$ (10 pts). \n",
    "\n",
    "We implement this start point strategy in the function `init_ls()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol.**\n",
    "\n",
    "To derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$, we can set $$0 = \\frac{d}{d\\Sigma} \\sum_{i=1}^n\\left\\|{r}_i^{(0)} {r}_i^{(0) T}-{Z}_{{i}} {Z}_{{i}}^{{T}}\\right\\|_F^2$$ then we have,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= \\frac{d}{d\\Sigma} \\sum_{i=1}^n \\operatorname{tr}\\left[\\left({r}_i^{(0)} {r}_i^{(0) T}-{Z}_{{i}} {\\Sigma} {Z}_{{i}}^{{T}}\\right)\\left({r}_i^{(0)} {r}_i^{(0) T}-{Z}_{{i}} \\Sigma {Z}_{{i}}^{{T}}\\right)^T\\right] \\\\\n",
    "0 &= \\frac{d}{d\\Sigma} \\sum_{i=1}^n \\left[\\operatorname { t r } \\left(\\left({r}_i^{(0)} {r}_i^{(0) T} {r}_i^{(0)} {r}_i^{(0) T}-2 \\operatorname{tr}\\left({Z}_{{i}} {\\Sigma} {Z}_{{i}}^{{T}} {r}_i^{(0)} {r}_i^{(0) T}\\right)+\\operatorname{tr}\\left({Z}_{{i}} {\\Sigma} {Z}_{{i}}^{{T}} {Z}_{{i}} {\\Sigma} {Z}_{{i}}^{{T}}\\right)\\right]\\right.\\right. \\\\\n",
    "0 &= \\sum_{i=1}^n\\left[-2\\left({Z}_{{i}}^T {r}_i^{(0)} {r}_i^{(0) T} {Z}_{{i}}\\right)+2\\left({Z}_{{i}}^T {Z}_{{i}} \\Sigma {Z}_{{i}}^T {Z}_{{i}}\\right)\\right] \\\\\n",
    "\\sum_{i=1}^n {Z}_{{i}}^T {r}_i^{(0)} {r}_i^{(0) T} {Z}_{{i}}&=\\sum_{i=1}^n {Z}_{{i}}^T {Z}_{{i}} \\Sigma {Z}_{{i}}^T {Z}_{{i}} \\\\\n",
    "\\sum_{i=1}^n\\left({Z}_{{i}}^T {r}_i^{(0)} \\otimes {Z}_{{i}}^T {r}_i^{(0)}\\right)&=\\sum_{i=1}^n\\left({Z}_{{i}}^T {Z}_i \\otimes {Z}_{{i}}^T {Z}_i\\right) \\operatorname{vec} \\Sigma^{(0)}\\\\\n",
    "\\operatorname{vec} {\\Sigma}^{(0)}&=\\left(\\sum_{i=1}^n {Z}_i^T {Z}_i \\otimes {Z}_i^T {Z}_i\\right)^{-1}\\left(\\sum_{i=1}^n {Z}_i^T {r}_i \\otimes {Z}_i^T {r}_i\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    \n",
    "    ##### m.β ######\n",
    "    m.xtx .= T(0)\n",
    "    m.xty .= T(0)\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        m.xtx .+= m.data[i].xtx\n",
    "        m.xty .+= m.data[i].xty\n",
    "    end\n",
    "    LAPACK.potrf!('L', m.xtx)\n",
    "    LAPACK.potrs!('L', m.xtx, m.xty)\n",
    "    m.β .= m.xty\n",
    "    \n",
    "    ##### σ^2 and L ##### \n",
    "    rsum = T(0)\n",
    "    nsum = T(0)\n",
    "    m.ztr2 .= T(0)\n",
    "    m.ztz2 .= T(0)\n",
    "\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        #σ2\n",
    "        obs = m.data[i]\n",
    "        BLAS.gemv!('N', T(1), obs.xtx, m.β, T(0), obs.storage_p)\n",
    "        rsum += obs.yty - 2 * dot(m.β, obs.xty) + dot(m.β, obs.storage_p)\n",
    "        nsum += size(obs.X, 1)\n",
    "        \n",
    "        #Σ\n",
    "        BLAS.gemv!('N', T(1), obs.ztx, m.β, T(0), obs.storage_q)\n",
    "        obs.storage_q .= obs.zty - obs.storage_q\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_q, obs.storage_q, T(0), obs.storage_qq)\n",
    "        m.ztr2 .+= vec(obs.storage_qq)\n",
    "        m.ztz2 .+= kron(obs.ztz, obs.ztz)\n",
    "    end\n",
    "    \n",
    "    m.σ² .= rsum / nsum\n",
    "    LAPACK.potrf!('L', m.ztz2)\n",
    "    LAPACK.potrs!('L', m.ztz2, m.ztr2)\n",
    "    m.L .= reshape(m.ztr2, q, q)\n",
    "    LAPACK.potrf!('L', m.L)\n",
    "    \n",
    "    m\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.362604934104436e6\n",
      "lmm.β = [0.18207934611476334, 6.500480700993719, -3.497910784209159, 1.001113296229795, 5.0002519857919285]\n",
      "lmm.σ² = [5.709004733413663]\n",
      "lmm.L = [1.4069222734993234 0.07258461003916693 0.05717147035274016; 0.051591059013255315 1.131979211870369 -0.07707942768978573; 0.04063584138912108 -0.06994463586493149 0.9718256360134828]\n"
     ]
    }
   ],
   "source": [
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Your start points should have a log-likelihood larger than -3.3627e6 (10 pts). The points you get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.3627e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "The start point should be computed quickly. Otherwise there is no point using it as a starting point. My median run time is 175μs. You get full credit (10 pts) if the median run time is within 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 7801 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m441.500 μs\u001b[22m\u001b[39m … \u001b[35m  8.718 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 91.54%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m558.200 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m638.482 μs\u001b[22m\u001b[39m ± \u001b[32m543.354 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m7.52% ±  8.25%\n",
       "\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m▂\u001b[39m\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  442 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       1.13 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m875.09 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m4002\u001b[39m."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. NLP via MathOptInterface.jl\n",
    "\n",
    "We define the NLP problem using the modelling tool [MathOptInterface.jl](https://github.com/jump-dev/MathOptInterface.jl). Start-up code is given below. Modify if necessary to accomodate your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver=Ipopt.Optimizer())\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m :: LmmModel{T},\n",
    "        solver = Ipopt.Optimizer()\n",
    "    ) where T <: AbstractFloat\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    # prep the MOI\n",
    "    MOI.empty!(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # q diagonal entries of Cholesky factor L should be >= 0\n",
    "    # σ² should be >= 0\n",
    "    lb   = fill(0.0, q + 1)\n",
    "    ub   = fill(Inf, q + 1)\n",
    "    NLPBlock = MOI.NLPBlockData(MOI.NLPBoundsPair.(lb, ub), m, true)\n",
    "    MOI.set(solver, MOI.NLPBlock(), NLPBlock)\n",
    "    # start point\n",
    "    params = MOI.add_variables(solver, npar)    \n",
    "    par0   = Vector{T}(undef, npar)\n",
    "    modelpar_to_optimpar!(par0, m)    \n",
    "    for i in 1:npar\n",
    "        MOI.set(solver, MOI.VariablePrimalStart(), params[i], par0[i])\n",
    "    end\n",
    "    MOI.set(solver, MOI.ObjectiveSense(), MOI.MAX_SENSE)\n",
    "    # optimize\n",
    "    MOI.optimize!(solver)\n",
    "    optstat = MOI.get(solver, MOI.TerminationStatus())\n",
    "    optstat in (MOI.LOCALLY_SOLVED, MOI.ALMOST_LOCALLY_SOLVED) || \n",
    "        @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    xsol = [MOI.get(solver, MOI.VariablePrimal(), MOI.VariableIndex(i)) for i in 1:npar]\n",
    "    optimpar_to_modelpar!(m, xsol)\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(par, m.β)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ²\n",
    "    par[end] = m.σ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(m.β, 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # σ²\n",
    "    m.σ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MOI.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in MOI.features_available(m))\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MOI.features_available(m::LmmModel) = [:Grad, :Hess, :Jac]\n",
    "\n",
    "function MOI.eval_objective(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MOI.eval_objective_gradient(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt β\n",
    "    copyto!(grad, m.∇β)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.∇L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to σ²\n",
    "    grad[end] = m.∇σ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "function MOI.eval_constraint(m::LmmModel, g, par)\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    gidx   = 1\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        if i == j\n",
    "            g[gidx] = par[offset]\n",
    "            gidx   += 1\n",
    "        end\n",
    "        offset += 1\n",
    "    end\n",
    "    g[end] = par[end]\n",
    "    g\n",
    "end\n",
    "\n",
    "function MOI.jacobian_structure(m::LmmModel)\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    row  = collect(1:(q + 1))\n",
    "    col  = Int[]\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        (i == j) && push!(col, offset)\n",
    "        offset += 1\n",
    "    end\n",
    "    push!(col, offset)\n",
    "    [(row[i], col[i]) for i in 1:length(row)]\n",
    "end\n",
    "\n",
    "MOI.eval_constraint_jacobian(m::LmmModel, J, par) = fill!(J, 1)\n",
    "\n",
    "function MOI.hessian_lagrangian_structure(m::LmmModel)\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)    \n",
    "    q◺   = ◺(q)\n",
    "    # we work on the upper triangular part of the Hessian\n",
    "    arr1 = Vector{Int}(undef, ◺(p) + ◺(q◺) + q◺ + 1)\n",
    "    arr2 = Vector{Int}(undef, ◺(p) + ◺(q◺) + q◺ + 1)\n",
    "    # Hββ block\n",
    "    idx  = 1    \n",
    "    for j in 1:p, i in 1:j\n",
    "        arr1[idx] = i\n",
    "        arr2[idx] = j\n",
    "        idx      += 1\n",
    "    end\n",
    "    # HLL block\n",
    "    for j in 1:q◺, i in 1:j\n",
    "        arr1[idx] = p + i\n",
    "        arr2[idx] = p + j\n",
    "        idx      += 1\n",
    "    end\n",
    "    # HLσ² block\n",
    "    for i in (p + 1):(p + q◺)\n",
    "        arr1[idx] = i\n",
    "        arr2[idx] = p + q◺ + 1\n",
    "        idx      += 1\n",
    "    end\n",
    "    # Hσ²σ² block\n",
    "    arr1[idx] = p + q◺ + 1\n",
    "    arr2[idx] = p + q◺ + 1\n",
    "    [(arr1[i], arr2[i]) for i in 1:length(arr1)]\n",
    "end\n",
    "\n",
    "function MOI.eval_hessian_lagrangian(\n",
    "        m   :: LmmModel, \n",
    "        H   :: AbstractVector{T},\n",
    "        par :: AbstractVector{T}, \n",
    "        σ   :: T, \n",
    "        μ   :: AbstractVector{T}\n",
    "    ) where {T}    \n",
    "    p  = size(m.data[1].X, 2)\n",
    "    q  = size(m.data[1].Z, 2)    \n",
    "    q◺ = ◺(q)\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, true, true)\n",
    "    # Hββ block\n",
    "    idx = 1    \n",
    "    @inbounds for j in 1:p, i in 1:j\n",
    "        H[idx] = m.Hββ[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # HLL block\n",
    "    @inbounds for j in 1:q◺, i in 1:j\n",
    "        H[idx] = m.HLL[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # HLσ² block\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        H[idx] = m.Hσ²L[i, j]\n",
    "        idx   += 1\n",
    "    end\n",
    "    # Hσ²σ² block\n",
    "    H[idx] = m.Hσ²σ²[1, 1]\n",
    "    lmul!(σ, H)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. (20 pts) Test drive\n",
    "\n",
    "Now we can run any NLP solver (supported by MathOptInterface.jl) to compute the MLE. For grading purpose, let's use the `:LD_MMA` ([Method of Moving Asymptotes](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#mma-method-of-moving-asymptotes-and-ccsa)) algorithm in NLopt.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.362604934104436e6\n",
      "\n",
      "  0.956957 seconds (1.18 M allocations: 78.514 MiB, 2.32% gc time, 83.89% compilation time)\n",
      "objective value at solution: -2.840075956762544e6)\n",
      "solution values:\n",
      "lmm.β = [0.18173470989466922, 6.500383304372868, -3.499864658327308, 0.9997120347011923, 4.9992296875077225]\n",
      "lmm.σ² = [1.4987379557095732]\n",
      "lmm.L * transpose(lmm.L) = [1.5776129180819056 0.028608962562930315 0.025242504643198493; 0.028608962562930315 1.179773518906685 -0.04424120703768006; 0.025242504643198493 -0.04424120703768006 0.962766739398985]\n",
      "gradient @ solution:\n",
      "lmm.∇β = [0.06795050012899395, 0.3270457416028929, 0.4101125694586685, 0.03336844664084282, 0.007242655691538147]\n",
      "lmm.∇σ² = [-1.260039275325333]\n",
      "lmm.∇L = [0.19405601975067965 16.649425311040467 17.03453861314172; -0.039002431596475415 -2.141502391046919 -38.57985589991842; 0.021886673557233727 0.06092317502676984 -4.89631676377129]\n",
      "norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²]) = 5.520140697672979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.520140697672979"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "# NLopt (LD_MMA) obj. val = -2.8400587866501966e6\n",
    "NLopt_solver = NLopt.Optimizer()\n",
    "MOI.set(NLopt_solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "@time fit!(lmm, NLopt_solver)\n",
    "\n",
    "println(\"objective value at solution: $(logl!(lmm)))\")\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L\n",
    "@show norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "You get 10 points if the following code does not throw `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: logl!(lmm) > -2.840059e6",
     "output_type": "error",
     "traceback": [
      "AssertionError: logl!(lmm) > -2.840059e6",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[53]:2"
     ]
    }
   ],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.840059e6\n",
    "# gradient at solution should be small enough\n",
    "@assert norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²]) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "My median run time is 50ms. You get 10 points if your median time is within 1s(=1000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 33 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m148.913 ms\u001b[22m\u001b[39m … \u001b[35m156.220 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m150.989 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m151.583 ms\u001b[22m\u001b[39m ± \u001b[32m  1.963 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m▃\u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▁\u001b[34m█\u001b[39m\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[32m▇\u001b[39m\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m \u001b[39m▁\n",
       "  149 ms\u001b[90m           Histogram: frequency by time\u001b[39m          156 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m26.41 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m799\u001b[39m."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLopt_solver = NLopt.Optimizer()\n",
    "MOI.set(NLopt_solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "bm_mma = @benchmark fit!($lmm, $(NLopt_solver)) setup=(init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_mma).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. (10 pts) Gradient free vs gradient-based methods\n",
    "\n",
    "Advantage of using a modelling tool such as MathOptInterface.jl is that we can easily switch the backend solvers. For a research problem, we never know beforehand which solver works best. \n",
    "\n",
    "Try different solvers in the NLopt.jl and Ipopt.jl packages. Compare the results in terms of run times (the shorter the better), objective values at solution (the larger the better), and gradients at solution (closer to 0 the better). Summarize what you find.\n",
    "\n",
    "See this [page](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/) for the descriptions of algorithms in NLopt.\n",
    "\n",
    "Documentation for the Ipopt can be found [here](https://coin-or.github.io/Ipopt/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `◺` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `◺` not defined",
      "",
      "Stacktrace:",
      "  [1] hessian_lagrangian_structure(m::LmmModel{Float64})",
      "    @ Main .\\In[47]:171",
      "  [2] hessian_lagrangian_structure(model::Ipopt.Optimizer)",
      "    @ Ipopt C:\\Users\\larry\\.julia\\packages\\Ipopt\\kZAqe\\src\\MOI_wrapper.jl:734",
      "  [3] _setup_model(model::Ipopt.Optimizer)",
      "    @ Ipopt C:\\Users\\larry\\.julia\\packages\\Ipopt\\kZAqe\\src\\MOI_wrapper.jl:774",
      "  [4] optimize!(model::Ipopt.Optimizer)",
      "    @ Ipopt C:\\Users\\larry\\.julia\\packages\\Ipopt\\kZAqe\\src\\MOI_wrapper.jl:867",
      "  [5] fit!(m::LmmModel{Float64}, solver::Ipopt.Optimizer)",
      "    @ Main .\\In[47]:32",
      "  [6] var\"##core#342\"(lmm#332::LmmModel{Float64}, solver#333::Ipopt.Optimizer)",
      "    @ Main C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:489",
      "  [7] var\"##sample#343\"(::Tuple{LmmModel{Float64}, Ipopt.Optimizer}, __params::BenchmarkTools.Parameters)",
      "    @ Main C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:495",
      "  [8] _run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; verbose::Bool, pad::String, kwargs::Base.Pairs{Symbol, Integer, NTuple{4, Symbol}, NamedTuple{(:samples, :evals, :gctrial, :gcsample), Tuple{Int64, Int64, Bool, Bool}}})",
      "    @ BenchmarkTools C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:99",
      "  [9] #invokelatest#2",
      "    @ .\\essentials.jl:818 [inlined]",
      " [10] invokelatest",
      "    @ .\\essentials.jl:813 [inlined]",
      " [11] #run_result#45",
      "    @ C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:34 [inlined]",
      " [12] run_result",
      "    @ C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:34 [inlined]",
      " [13] run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, kwargs::Base.Pairs{Symbol, Integer, NTuple{5, Symbol}, NamedTuple{(:verbose, :samples, :evals, :gctrial, :gcsample), Tuple{Bool, Int64, Int64, Bool, Bool}}})",
      "    @ BenchmarkTools C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:117",
      " [14] run (repeats 2 times)",
      "    @ C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:117 [inlined]",
      " [15] #warmup#54",
      "    @ C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:169 [inlined]",
      " [16] warmup(item::BenchmarkTools.Benchmark)",
      "    @ BenchmarkTools C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:168",
      " [17] macro expansion",
      "    @ C:\\Users\\larry\\.julia\\packages\\BenchmarkTools\\0owsb\\src\\execution.jl:393 [inlined]",
      " [18] top-level scope",
      "    @ In[56]:34"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\"NLopt (LN_COBYLA, gradient free)\", \"NLopt (LD_MMA, gradient-based)\", \n",
    "    \"Ipopt (L-BFGS)\"]\n",
    "\n",
    "function setup_solver(s::String)\n",
    "    if s == \"NLopt (LN_COBYLA, gradient free)\"\n",
    "        solver = NLopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LN_COBYLA)\n",
    "    elseif s == \"NLopt (LD_MMA, gradient-based)\"\n",
    "        solver = NLopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"algorithm\"), :LD_MMA)\n",
    "    elseif s == \"Ipopt (L-BFGS)\"\n",
    "        solver = Ipopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"print_level\"), 0)\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"hessian_approximation\"), \"limited-memory\")\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"tol\"), 1e-6)\n",
    "    elseif s == \"Ipopt (use FIM)\"\n",
    "        # Ipopt (use Hessian) obj val = -2.8400587866468e6\n",
    "        solver = Ipopt.Optimizer()\n",
    "        MOI.set(solver, MOI.RawOptimizerAttribute(\"print_level\"), 0)        \n",
    "    else\n",
    "        error(\"unrecognized solver $s\")\n",
    "    end\n",
    "    solver\n",
    "end\n",
    "\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for i in 1:length(solvers)\n",
    "    solver = setup_solver(solvers[i])\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = norm([lmm.∇β; vec(LowerTriangular(lmm.∇L)); lmm.∇σ²])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────┬─────────┬───────────────────┬───────────────┐\n",
      "│\u001b[1m                           Solver \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m          Log-Like \u001b[0m│\u001b[1m Gradiant Norm \u001b[0m│\n",
      "├──────────────────────────────────┼─────────┼───────────────────┼───────────────┤\n",
      "│ NLopt (LN_COBYLA, gradient free) │    0.60 │ -2840080.63551096 │  364.42460726 │\n",
      "│   NLopt (LD_MMA, gradient-based) │    0.15 │ -2840075.95676254 │    5.52014070 │\n",
      "│                   Ipopt (L-BFGS) │    0.00 │        0.00000000 │    0.00000000 │\n",
      "└──────────────────────────────────┴─────────┴───────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# display results\n",
    "pretty_table(\n",
    "    hcat(solvers, runtime, objvals, gradnrm),\n",
    "    header = [\"Solver\", \"Runtime\", \"Log-Like\", \"Gradiant Norm\"],\n",
    "    formatters = (ft_printf(\"%5.2f\", 2), ft_printf(\"%8.8f\", 3:4))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. (10 pts) Compare with existing art\n",
    "\n",
    "Let's compare our method with lme4 package in R and MixedModels.jl package in Julia. Both lme4 and MixedModels.jl are developed mainly by Doug Bates. Summarize what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"My method\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.840075956762544e6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = setup_solver(\"NLopt (LD_MMA, gradient-based)\")\n",
    "bm_257 = @benchmark fit!($lmm, $solver) setup=(init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lme4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1744977\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m8\u001b[39m\n",
      "\u001b[36m--\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m--------------------------------------------------------\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[32mdbl\u001b[39m (8): ID, Y, X1, X2, X3, X4, Z1, Z2\n",
      "\n",
      "\u001b[36mi\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mi\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ RCall C:\\Users\\larry\\.julia\\packages\\RCall\\LWzAQ\\src\\io.jl:172\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{VecSxp}\n",
       "\u001b[38;5;246m# A tibble: 1,744,977 x 8\u001b[39m\n",
       "      ID        Y     X1      X2     X3      X4      Z1      Z2\n",
       "   \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m  \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m  \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
       "\u001b[38;5;250m 1\u001b[39m     1   9.52    0.202 -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m463\u001b[39m   0.798  0.734   0.685  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m570\u001b[39m \n",
       "\u001b[38;5;250m 2\u001b[39m     1  24.4     1.59  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m95\u001b[39m    1.20   1.43    1.64    0.369 \n",
       "\u001b[38;5;250m 3\u001b[39m     1  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m99\u001b[39m    0.378 -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m36\u001b[4m7\u001b[24m\u001b[39m  1.63  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m15\u001b[39m   -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m818\u001b[39m   2.83  \n",
       "\u001b[38;5;250m 4\u001b[39m     1 -\u001b[31m17\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m    -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m88\u001b[39m   0.375  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m498\u001b[39m -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m253\u001b[39m   1.56    1.68  \n",
       "\u001b[38;5;250m 5\u001b[39m     1  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m70\u001b[4m4\u001b[24m\u001b[39m  0.658 -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m165\u001b[39m   0.780 -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m23\u001b[39m   -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m28\u001b[4m8\u001b[24m\u001b[39m -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m9\u001b[39m  \n",
       "\u001b[38;5;250m 6\u001b[39m     1  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m853\u001b[39m   0.458 -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m313\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m512\u001b[39m -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m800\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m331\u001b[39m   1.98  \n",
       "\u001b[38;5;250m 7\u001b[39m     1  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m80\u001b[39m    0.220  0.328   1.32  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m1\u001b[39m   -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m363\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m70\u001b[4m3\u001b[24m\u001b[39m\n",
       "\u001b[38;5;250m 8\u001b[39m     1   5.88    1.30   0.889  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m854\u001b[39m  0.071\u001b[4m4\u001b[24m -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m658\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m0\u001b[39m\u001b[31m33\u001b[4m9\u001b[24m\u001b[39m\n",
       "\u001b[38;5;250m 9\u001b[39m     1  -\u001b[31m9\u001b[39m\u001b[31m.\u001b[39m\u001b[31m21\u001b[39m   -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m43\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m522\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m119\u001b[39m -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m580\u001b[39m  -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m155\u001b[39m  -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m89\u001b[39m  \n",
       "\u001b[38;5;250m10\u001b[39m     1 -\u001b[31m11\u001b[39m\u001b[31m.\u001b[39m\u001b[31m3\u001b[39m    -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m468\u001b[39m -\u001b[31m0\u001b[39m\u001b[31m.\u001b[39m\u001b[31m700\u001b[39m   0.872 -\u001b[31m1\u001b[39m\u001b[31m.\u001b[39m\u001b[31m82\u001b[39m    1.80    0.492 \n",
       "\u001b[38;5;246m# i 1,744,967 more rows\u001b[39m\n",
       "\u001b[38;5;246m# i Use `print(n = ...)` to see more rows\u001b[39m\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RObject{RealSxp}\n",
       "   user  system elapsed \n",
       "  43.90    8.46   82.81 \n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MixedModels.jl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1744977×8 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1744952 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">ID</th><th style = \"text-align: left;\">Y</th><th style = \"text-align: left;\">X1</th><th style = \"text-align: left;\">X2</th><th style = \"text-align: left;\">X3</th><th style = \"text-align: left;\">X4</th><th style = \"text-align: left;\">Z1</th><th style = \"text-align: left;\">Z2</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">9.52102</td><td style = \"text-align: right;\">0.201821</td><td style = \"text-align: right;\">-0.463234</td><td style = \"text-align: right;\">0.797731</td><td style = \"text-align: right;\">0.73357</td><td style = \"text-align: right;\">0.685476</td><td style = \"text-align: right;\">-0.569622</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">24.4063</td><td style = \"text-align: right;\">1.58557</td><td style = \"text-align: right;\">-1.94608</td><td style = \"text-align: right;\">1.19787</td><td style = \"text-align: right;\">1.43149</td><td style = \"text-align: right;\">1.63962</td><td style = \"text-align: right;\">0.369053</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-1.99215</td><td style = \"text-align: right;\">0.378332</td><td style = \"text-align: right;\">-0.0367002</td><td style = \"text-align: right;\">1.63072</td><td style = \"text-align: right;\">-1.15031</td><td style = \"text-align: right;\">-0.817843</td><td style = \"text-align: right;\">2.83422</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-17.4233</td><td style = \"text-align: right;\">-1.8826</td><td style = \"text-align: right;\">0.374561</td><td style = \"text-align: right;\">-0.49786</td><td style = \"text-align: right;\">-0.253248</td><td style = \"text-align: right;\">1.56433</td><td style = \"text-align: right;\">1.67857</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.0704245</td><td style = \"text-align: right;\">0.658283</td><td style = \"text-align: right;\">-0.165487</td><td style = \"text-align: right;\">0.77951</td><td style = \"text-align: right;\">-1.22763</td><td style = \"text-align: right;\">-0.0287779</td><td style = \"text-align: right;\">-1.09172</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.853357</td><td style = \"text-align: right;\">0.457784</td><td style = \"text-align: right;\">-0.313387</td><td style = \"text-align: right;\">-0.512299</td><td style = \"text-align: right;\">-0.800278</td><td style = \"text-align: right;\">-0.330632</td><td style = \"text-align: right;\">1.97609</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-1.80061</td><td style = \"text-align: right;\">0.220461</td><td style = \"text-align: right;\">0.327879</td><td style = \"text-align: right;\">1.32209</td><td style = \"text-align: right;\">-1.01336</td><td style = \"text-align: right;\">-0.362947</td><td style = \"text-align: right;\">-0.0703055</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">5.88119</td><td style = \"text-align: right;\">1.30135</td><td style = \"text-align: right;\">0.88884</td><td style = \"text-align: right;\">-0.853941</td><td style = \"text-align: right;\">0.0714372</td><td style = \"text-align: right;\">-0.658202</td><td style = \"text-align: right;\">-0.0338648</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-9.20504</td><td style = \"text-align: right;\">-1.43248</td><td style = \"text-align: right;\">-0.521638</td><td style = \"text-align: right;\">-0.119287</td><td style = \"text-align: right;\">-0.579596</td><td style = \"text-align: right;\">-0.154869</td><td style = \"text-align: right;\">-1.88707</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-11.2909</td><td style = \"text-align: right;\">-0.46827</td><td style = \"text-align: right;\">-0.699709</td><td style = \"text-align: right;\">0.871668</td><td style = \"text-align: right;\">-1.81529</td><td style = \"text-align: right;\">1.79726</td><td style = \"text-align: right;\">0.492339</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">8.9367</td><td style = \"text-align: right;\">2.00212</td><td style = \"text-align: right;\">1.1577</td><td style = \"text-align: right;\">-0.973746</td><td style = \"text-align: right;\">-0.784991</td><td style = \"text-align: right;\">-2.12648</td><td style = \"text-align: right;\">-1.68961</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.496904</td><td style = \"text-align: right;\">-0.480658</td><td style = \"text-align: right;\">0.1732</td><td style = \"text-align: right;\">-1.27733</td><td style = \"text-align: right;\">0.571911</td><td style = \"text-align: right;\">-0.802521</td><td style = \"text-align: right;\">0.584438</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">1</td><td style = \"text-align: right;\">-0.139879</td><td style = \"text-align: right;\">0.597359</td><td style = \"text-align: right;\">-0.894785</td><td style = \"text-align: right;\">-0.904349</td><td style = \"text-align: right;\">-1.10844</td><td style = \"text-align: right;\">0.106724</td><td style = \"text-align: right;\">-0.5961</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744966</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">14.3459</td><td style = \"text-align: right;\">2.53537</td><td style = \"text-align: right;\">-0.385281</td><td style = \"text-align: right;\">0.374004</td><td style = \"text-align: right;\">-1.06646</td><td style = \"text-align: right;\">-0.881868</td><td style = \"text-align: right;\">-0.524693</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744967</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-1.86731</td><td style = \"text-align: right;\">-0.156314</td><td style = \"text-align: right;\">-0.359602</td><td style = \"text-align: right;\">-1.00392</td><td style = \"text-align: right;\">-0.463548</td><td style = \"text-align: right;\">-1.40846</td><td style = \"text-align: right;\">0.289986</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744968</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-3.12427</td><td style = \"text-align: right;\">-0.393863</td><td style = \"text-align: right;\">-0.491481</td><td style = \"text-align: right;\">1.55365</td><td style = \"text-align: right;\">-0.776784</td><td style = \"text-align: right;\">0.688556</td><td style = \"text-align: right;\">-1.16616</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744969</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-3.20226</td><td style = \"text-align: right;\">-0.126944</td><td style = \"text-align: right;\">0.601992</td><td style = \"text-align: right;\">-0.960992</td><td style = \"text-align: right;\">-0.300833</td><td style = \"text-align: right;\">1.07374</td><td style = \"text-align: right;\">0.649807</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744970</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">0.0979076</td><td style = \"text-align: right;\">-0.0942822</td><td style = \"text-align: right;\">-0.017406</td><td style = \"text-align: right;\">1.01639</td><td style = \"text-align: right;\">-0.111906</td><td style = \"text-align: right;\">-0.708675</td><td style = \"text-align: right;\">0.0446113</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744971</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">6.84005</td><td style = \"text-align: right;\">0.286095</td><td style = \"text-align: right;\">-0.995131</td><td style = \"text-align: right;\">-0.293129</td><td style = \"text-align: right;\">0.193649</td><td style = \"text-align: right;\">-1.03079</td><td style = \"text-align: right;\">-1.9487</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744972</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">8.44826</td><td style = \"text-align: right;\">-0.0458988</td><td style = \"text-align: right;\">-0.00257329</td><td style = \"text-align: right;\">-1.31491</td><td style = \"text-align: right;\">1.51216</td><td style = \"text-align: right;\">0.0174882</td><td style = \"text-align: right;\">0.530319</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744973</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-5.13234</td><td style = \"text-align: right;\">-1.11932</td><td style = \"text-align: right;\">-1.06951</td><td style = \"text-align: right;\">-0.339256</td><td style = \"text-align: right;\">-0.500339</td><td style = \"text-align: right;\">0.737606</td><td style = \"text-align: right;\">1.58472</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744974</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-0.530812</td><td style = \"text-align: right;\">-0.850859</td><td style = \"text-align: right;\">0.00500998</td><td style = \"text-align: right;\">-0.274531</td><td style = \"text-align: right;\">1.01461</td><td style = \"text-align: right;\">0.382234</td><td style = \"text-align: right;\">-0.435956</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744975</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-8.88652</td><td style = \"text-align: right;\">-0.762326</td><td style = \"text-align: right;\">0.154867</td><td style = \"text-align: right;\">-0.933923</td><td style = \"text-align: right;\">-0.931906</td><td style = \"text-align: right;\">0.797417</td><td style = \"text-align: right;\">0.888702</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744976</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">-1.42159</td><td style = \"text-align: right;\">-0.672599</td><td style = \"text-align: right;\">-1.25194</td><td style = \"text-align: right;\">-0.518835</td><td style = \"text-align: right;\">-0.0647723</td><td style = \"text-align: right;\">-1.48993</td><td style = \"text-align: right;\">-0.00689683</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1744977</td><td style = \"text-align: left;\">1000</td><td style = \"text-align: right;\">3.79889</td><td style = \"text-align: right;\">-1.21758</td><td style = \"text-align: right;\">-1.49033</td><td style = \"text-align: right;\">1.04599</td><td style = \"text-align: right;\">0.459857</td><td style = \"text-align: right;\">0.628323</td><td style = \"text-align: right;\">0.856971</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& ID & Y & X1 & X2 & X3 & X4 & Z1 & Z2\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 9.52102 & 0.201821 & -0.463234 & 0.797731 & 0.73357 & 0.685476 & -0.569622 \\\\\n",
       "\t2 & 1 & 24.4063 & 1.58557 & -1.94608 & 1.19787 & 1.43149 & 1.63962 & 0.369053 \\\\\n",
       "\t3 & 1 & -1.99215 & 0.378332 & -0.0367002 & 1.63072 & -1.15031 & -0.817843 & 2.83422 \\\\\n",
       "\t4 & 1 & -17.4233 & -1.8826 & 0.374561 & -0.49786 & -0.253248 & 1.56433 & 1.67857 \\\\\n",
       "\t5 & 1 & -0.0704245 & 0.658283 & -0.165487 & 0.77951 & -1.22763 & -0.0287779 & -1.09172 \\\\\n",
       "\t6 & 1 & -0.853357 & 0.457784 & -0.313387 & -0.512299 & -0.800278 & -0.330632 & 1.97609 \\\\\n",
       "\t7 & 1 & -1.80061 & 0.220461 & 0.327879 & 1.32209 & -1.01336 & -0.362947 & -0.0703055 \\\\\n",
       "\t8 & 1 & 5.88119 & 1.30135 & 0.88884 & -0.853941 & 0.0714372 & -0.658202 & -0.0338648 \\\\\n",
       "\t9 & 1 & -9.20504 & -1.43248 & -0.521638 & -0.119287 & -0.579596 & -0.154869 & -1.88707 \\\\\n",
       "\t10 & 1 & -11.2909 & -0.46827 & -0.699709 & 0.871668 & -1.81529 & 1.79726 & 0.492339 \\\\\n",
       "\t11 & 1 & 8.9367 & 2.00212 & 1.1577 & -0.973746 & -0.784991 & -2.12648 & -1.68961 \\\\\n",
       "\t12 & 1 & -0.496904 & -0.480658 & 0.1732 & -1.27733 & 0.571911 & -0.802521 & 0.584438 \\\\\n",
       "\t13 & 1 & -0.139879 & 0.597359 & -0.894785 & -0.904349 & -1.10844 & 0.106724 & -0.5961 \\\\\n",
       "\t14 & 1 & -8.71947 & -0.511146 & -1.338 & -1.18855 & -1.8141 & 0.469028 & -0.172564 \\\\\n",
       "\t15 & 1 & -2.58409 & -0.597996 & 0.059796 & -0.655992 & 0.0857626 & -1.16351 & -0.240554 \\\\\n",
       "\t16 & 1 & -5.81281 & 0.411499 & 0.437668 & -0.725361 & -0.812424 & 1.99023 & -0.768594 \\\\\n",
       "\t17 & 1 & -0.596476 & 0.0463844 & 0.0538646 & 0.663663 & 0.51955 & 2.30564 & 0.4257 \\\\\n",
       "\t18 & 1 & -3.18641 & -1.49591 & 0.659436 & 0.956284 & 1.24877 & -0.824118 & 0.177151 \\\\\n",
       "\t19 & 1 & 1.56221 & -0.606838 & 1.08345 & 0.163253 & 1.84409 & -0.511557 & -0.994212 \\\\\n",
       "\t20 & 1 & 3.02138 & 0.0405701 & 0.0647205 & 1.22975 & -0.0210971 & -1.50206 & -0.414032 \\\\\n",
       "\t21 & 1 & 22.7553 & 2.22447 & 0.917703 & 0.621901 & 2.02733 & -0.0768099 & -0.0520619 \\\\\n",
       "\t22 & 1 & 3.70676 & 0.56003 & -1.49019 & 0.368436 & -1.24426 & -0.853134 & 0.712161 \\\\\n",
       "\t23 & 1 & -8.04279 & -0.823574 & -0.627208 & -0.956862 & -1.0348 & -0.873069 & -0.856131 \\\\\n",
       "\t24 & 1 & -2.33705 & -0.721538 & 0.362801 & 0.548455 & 0.894375 & 1.89845 & 1.32574 \\\\\n",
       "\t25 & 1 & 14.8916 & 0.626803 & -2.96424 & -0.747008 & 0.0964266 & 0.117514 & 1.64184 \\\\\n",
       "\t26 & 1 & 18.3082 & 0.770025 & -0.257942 & 0.513041 & 2.25855 & 0.571801 & -1.39796 \\\\\n",
       "\t27 & 1 & 1.19626 & 0.0607296 & 0.315506 & -0.279184 & 0.42981 & -0.546869 & 0.118436 \\\\\n",
       "\t28 & 1 & -15.2916 & -0.830792 & 2.3629 & -0.210138 & -0.659754 & -0.717479 & 0.223425 \\\\\n",
       "\t29 & 1 & 8.53769 & 1.02613 & -0.0021617 & -1.35241 & -0.0927837 & -0.666525 & -0.0567915 \\\\\n",
       "\t30 & 1 & -10.3919 & -0.465915 & 0.560413 & -1.41106 & -0.921584 & -1.17228 & -0.219102 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1744977×8 DataFrame\u001b[0m\n",
       "\u001b[1m     Row \u001b[0m│\u001b[1m ID     \u001b[0m\u001b[1m Y           \u001b[0m\u001b[1m X1         \u001b[0m\u001b[1m X2          \u001b[0m\u001b[1m X3        \u001b[0m\u001b[1m X4        \u001b[0m ⋯\n",
       "         │\u001b[90m String \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m ⋯\n",
       "─────────┼──────────────────────────────────────────────────────────────────────\n",
       "       1 │ 1         9.52102     0.201821   -0.463234     0.797731   0.73357   ⋯\n",
       "       2 │ 1        24.4063      1.58557    -1.94608      1.19787    1.43149\n",
       "       3 │ 1        -1.99215     0.378332   -0.0367002    1.63072   -1.15031\n",
       "       4 │ 1       -17.4233     -1.8826      0.374561    -0.49786   -0.253248\n",
       "       5 │ 1        -0.0704245   0.658283   -0.165487     0.77951   -1.22763   ⋯\n",
       "       6 │ 1        -0.853357    0.457784   -0.313387    -0.512299  -0.800278\n",
       "       7 │ 1        -1.80061     0.220461    0.327879     1.32209   -1.01336\n",
       "       8 │ 1         5.88119     1.30135     0.88884     -0.853941   0.0714372\n",
       "       9 │ 1        -9.20504    -1.43248    -0.521638    -0.119287  -0.579596  ⋯\n",
       "      10 │ 1       -11.2909     -0.46827    -0.699709     0.871668  -1.81529\n",
       "      11 │ 1         8.9367      2.00212     1.1577      -0.973746  -0.784991\n",
       "    ⋮    │   ⋮          ⋮           ⋮            ⋮           ⋮          ⋮      ⋱\n",
       " 1744968 │ 1000     -3.12427    -0.393863   -0.491481     1.55365   -0.776784\n",
       " 1744969 │ 1000     -3.20226    -0.126944    0.601992    -0.960992  -0.300833  ⋯\n",
       " 1744970 │ 1000      0.0979076  -0.0942822  -0.017406     1.01639   -0.111906\n",
       " 1744971 │ 1000      6.84005     0.286095   -0.995131    -0.293129   0.193649\n",
       " 1744972 │ 1000      8.44826    -0.0458988  -0.00257329  -1.31491    1.51216\n",
       " 1744973 │ 1000     -5.13234    -1.11932    -1.06951     -0.339256  -0.500339  ⋯\n",
       " 1744974 │ 1000     -0.530812   -0.850859    0.00500998  -0.274531   1.01461\n",
       " 1744975 │ 1000     -8.88652    -0.762326    0.154867    -0.933923  -0.931906\n",
       " 1744976 │ 1000     -1.42159    -0.672599   -1.25194     -0.518835  -0.0647723\n",
       " 1744977 │ 1000      3.79889    -1.21758    -1.49033      1.04599    0.459857  ⋯\n",
       "\u001b[36m                                              2 columns and 1744956 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = CSV.File(\"lmm_data.csv\", types = Dict(1=>String)) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ ProgressMeter C:\\Users\\larry\\.julia\\packages\\ProgressMeter\\sN2xr\\src\\ProgressMeter.jl:618\u001b[39m\n",
      "\u001b[32mMinimizing 176 \t Time: 0:00:00 ( 1.32 ms/it)\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4722747"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), $testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 11 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m449.533 ms\u001b[22m\u001b[39m … \u001b[35m515.701 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m1.88% … 9.66%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m472.275 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m5.05%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m473.591 ms\u001b[22m\u001b[39m ± \u001b[32m 21.002 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m5.97% ± 2.38%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[34m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[32m \u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[34m█\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[32m▁\u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  450 ms\u001b[90m           Histogram: frequency by time\u001b[39m          516 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m841.83 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m6392\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{tabular}\n",
       "{l | r | r | r | r | r}\n",
       " & Est. & SE & z & p & $\\sigma_\\text{ID}$ \\\\\n",
       "\\hline\n",
       "(Intercept) & 0.1815 & 0.0445 & 4.08 & <1e-04 & 1.4082 \\\\\n",
       "X1 & 6.5004 & 0.0009 & 7000.80 & <1e-99 &   \\\\\n",
       "X2 & -3.4999 & 0.0009 & -3768.92 & <1e-99 &   \\\\\n",
       "X3 & 0.9997 & 0.0009 & 1078.16 & <1e-99 &   \\\\\n",
       "X4 & 4.9992 & 0.0009 & 5391.08 & <1e-99 &   \\\\\n",
       "Z2 &  &  &  &  & 0.9713 \\\\\n",
       "Z1 &  &  &  &  & 1.1319 \\\\\n",
       "Residual & 1.2242 &  &  &  &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "|             |    Est. |     SE |        z |      p |   σ_ID |\n",
       "|:----------- | -------:| ------:| --------:| ------:| ------:|\n",
       "| (Intercept) |  0.1815 | 0.0445 |     4.08 | <1e-04 | 1.4082 |\n",
       "| X1          |  6.5004 | 0.0009 |  7000.80 | <1e-99 |        |\n",
       "| X2          | -3.4999 | 0.0009 | -3768.92 | <1e-99 |        |\n",
       "| X3          |  0.9997 | 0.0009 |  1078.16 | <1e-99 |        |\n",
       "| X4          |  4.9992 | 0.0009 |  5391.08 | <1e-99 |        |\n",
       "| Z2          |         |        |          |        | 0.9713 |\n",
       "| Z1          |         |        |          |        | 1.1319 |\n",
       "| Residual    |  1.2242 |        |          |        |        |\n"
      ],
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " Y ~ 1 + X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)\n",
       "     logLik     -2 logLik        AIC           AICc          BIC      \n",
       " -2840058.7867  5680117.5734  5680141.5734  5680141.5736  5680290.0405\n",
       "\n",
       "Variance components:\n",
       "            Column   Variance Std.Dev.   Corr.\n",
       "ID       (Intercept)  1.982920 1.408162\n",
       "         Z1           1.281135 1.131873 +0.04\n",
       "         Z2           0.943437 0.971307 +0.04 -0.08\n",
       "Residual              1.498735 1.224228\n",
       " Number of obs: 1744977; levels of grouping factors: 1000\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────\n",
       "                 Coef.   Std. Error         z  Pr(>|z|)\n",
       "───────────────────────────────────────────────────────\n",
       "(Intercept)   0.181511  0.0444586        4.08    <1e-04\n",
       "X1            6.50038   0.00092852    7000.80    <1e-99\n",
       "X2           -3.49986   0.000928611  -3768.92    <1e-99\n",
       "X3            0.999712  0.000927237   1078.16    <1e-99\n",
       "X4            4.99923   0.000927316   5391.08    <1e-99\n",
       "───────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────┬─────────┬─────────────────┐\n",
      "│\u001b[1m         Method \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m        Log-Like \u001b[0m│\n",
      "├────────────────┼─────────┼─────────────────┤\n",
      "│      My method │    0.15 │ -2840075.956763 │\n",
      "│           lme4 │   82.81 │ -2840058.786648 │\n",
      "│ MixedModels.jl │    0.47 │ -2840058.786716 │\n",
      "└────────────────┴─────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "pretty_table(\n",
    "    hcat(method, runtime, loglike),\n",
    "    header = [\"Method\", \"Runtime\", \"Log-Like\"],\n",
    "    formatters = (ft_printf(\"%5.2f\", 2), ft_printf(\"%8.6f\", 3))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Be proud of yourself\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analysis on complex longitudinal data sets with millions of records. And you beat current software by XXX fold."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.9.0 (8 threads) 1.9.0",
   "language": "julia",
   "name": "julia-1.9.0-_8-threads_-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
