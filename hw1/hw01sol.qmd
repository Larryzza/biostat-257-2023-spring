---
title: Biostat/Biomath M257 Homework 1
subtitle: 'Due Apr 14 @ 11:59PM'
author: Zian ZHUANG (UID:405444165)
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
jupyter:
  jupytext:
    formats: 'ipynb,qmd'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Julia (16 threads) 1.8.5
    language: julia
    name: julia-_16-threads_-1.8
---

```{julia}
versioninfo()
```

```{julia}
using Pkg
Pkg.activate(pwd())
Pkg.instantiate()
Pkg.status()
using LoopVectorization, Random, CairoMakie, LinearAlgebra, BenchmarkTools
```

## Q1

**No handwritten homework reports are accepted for this course.** We work with Git/GitHub. Efficient and abundant use of Git, e.g., **frequent and well-documented** commits, is an important criterion for grading your homework.

1.  If you don't have a GitHub account, apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email.

2.  Create a **private** repository `biostat-257-2023-spring` and add `Hua-Zhou` and `parsajamshidian` (TA) as your collaborators.

3.  Top directories of the repository should be `hw1`, `hw2`, ... You may create other branches for developing your homework solutions; but the `master` branch will be your presentation area. Put your homework submission files (Jupyter notebook `.ipynb`, html converted from notebook, all code and data set to reproduce results) in the `master` branch.

4.  After each homework due date, teaching assistant and instructor will check out your `master` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your hw1 submission after deadline, penalty points will be deducted for late submission.

5.  Read the [style guide](https://github.com/invenia/BlueStyle) for Julia programming. Following rules in the style guide will be strictly enforced when grading: (1) four space indenting rule, (2) 92 charcter rule, (3) space after comma rule, (4) no space before comma rule, (5) space around binary operator rule.

### Sol.

1.  done

2.  done

3.  done

4.  done

5.  done

## Q2

Let's check whether floating-point numbers obey certain algebraic rules. For 2-5, one counter-example suffices.

1.  Associative rule for addition says `(x + y) + z == x + (y + z)`. Check association rule using `x = 0.1`, `y = 0.1` and `z = 1.0` in Julia. Explain what you find.

2.  Do floating-point numbers obey the associative rule for multiplication: `(x * y) * z == x * (y * z)`?

3.  Do floating-point numbers obey the distributive rule: `a * (x + y) == a * x + a * y`?

4.  Is `0 * x == 0` true for all floating-point number `x`?

5.  Is `x / a == x * (1 / a)` always true?

### 1.

```{julia}
x, y, z = 0.1, 0.1, 1.0
print("(x + y) + z = ", (x + y) + z, ";  x + (y + z) = ", x + (y + z))
(x + y) + z == x + (y + z)
```

We found that associative rule is invalid here. Explain: When we added two floating-point numbers with different magnitudes, computer did rounding and we loose the precision of the number with smaller magnitude. In `(x + y) + z` we rounded one time but `x + (y + z)` we did twice. Thus, they provided different answers. 

### 2.

```{julia}
x, y, z = 11.1, 0.1, 111.1
print("(x * y) * z = ", (x * y) * z, ";  x * (y * z) = ", x * (y * z))
(x * y) * z == x * (y * z)
```

No, floating-point numbers do not obey the associative rule for multiplication. The reason is same as above (rounding issues).

### 3.

```{julia}
x, y, z = 11.1, 0.1, 111.1
print("z * (x + y) = ", z * (x + y), ";  z * x + z * y = ", z * x + z * y)
(x * y) * z == x * (y * z)
```

No, floating-point numbers do not obey the distributive rule. The reason is same as above (rounding issues).

### 4.

```{julia}
0 * Inf == 0
```

We can see `0 * x == 0` is not always true for all floating-point number `x`.

### 5.

```{julia}
x, a = 1e-13, 1e10
print("x / a = ", x / a, ";  x * (1 / a) = ", x * (1 / a))
x / a == x * (1 / a)
```

Then we can see `x / a == x * (1 / a)` is not always true.

## Q3

Consider Julia function

```{julia}
function g(k)
    for i in 1:10
        k = 5k - 1
    end
    k
end
```

1.  Use `@code_llvm` to find the LLVM bitcode of compiled `g` with `Int64` input.\
2.  Use `@code_llvm` to find the LLVM bitcode of compiled `g` with `Float64` input.\
3.  Compare the bitcode from questions 1 and 2. What do you find?\
4.  Read Julia documentation on `@fastmath` and repeat the questions 1-3 on the function

```{julia}
function g_fastmath(k)  
    @fastmath for i in 1:10  
        k = 5k - 1
    end
    k
end
```

Explain what does the macro `@fastmath` do? And why are the bitcodes for `g` and `g_fastmath` with `Float64` input different? (Hint: Q2)

### 1.

```{julia}
@code_llvm g(1)
```

### 2.

```{julia}
@code_llvm g(1.1)
```

### 3.

We can see that although we didn’t specify a type annotation. LLVM bitcodes were still generated depending on the appropriate argument type. LLVM code shows that , compared to the Float type (20 steps),  computation for Int type is much simpler (2 steps).

### 4.

```{julia}
@code_llvm g_fastmath(1)
```

```{julia}
@code_llvm g_fastmath(1.1)
```

As we can tell from the llvm code, `fastmath` deals with the Float types in a more efficient way (2 steps) than the original method (20 steps). But we need to be careful when using this tool, since in document it states that this method

> Execute a transformed version of the expression, which calls functions that may violate strict IEEE semantics. This allows the fastest possible operation, but results are undefined – be careful when doing this, as it may change numerical results.


## Q4

Create the vector `x = (0.988, 0.989, 0.990, ..., 1.010, 1.011, 1.012)`.

1.  Plot the polynomial `y = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1` at points `x`.

2.  Plot the polynomial `y = (x - 1)^7` at points `x`.

3.  Explain what you found.

### 1.

```{julia}
x = [0.988:0.001:1.012...]
function f1(x::Real)
    x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1
end

lines(x, f1, color = :red, label = "polynomial 1")
axislegend()
current_figure()
```

### 2.

```{julia}
x = [0.988:0.001:1.012...]
function f2(x :: Real)
    (x - 1)^7
end
lines(x, f2, color = :blue, label = "polynomial 2")
axislegend()
current_figure()
```

### 3.

Here we plot `y = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1` and `y = (x - 1)^7` together:

```{julia}
lines(x, f1, color = :red, label = "polynomial 1")
lines!(x, f2, color = :blue, label = "polynomial 2")
axislegend()
current_figure()
```

We can find that `y = (x - 1)^7` is smoother than `y = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1`. Possible reason: `y = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1` did much more rounding than `y = (x - 1)^7` because it calculated each block separately (`x^7`, `7x^6`, ...) and lost more precision.

## Q5

1.  Show the **Sherman-Morrison formula** $$
     (\mathbf{A} + \mathbf{u} \mathbf{u}^T)^{-1} = \mathbf{A}^{-1} - \frac{1}{1 + \mathbf{u}^T \mathbf{A}^{-1} \mathbf{u}} \mathbf{A}^{-1} \mathbf{u} \mathbf{u}^T \mathbf{A}^{-1},
    $$ where $\mathbf{A} \in \mathbb{R}^{n \times n}$ is nonsingular and $\mathbf{u} \in \mathbb{R}^n$. This formula supplies the inverse of the symmetric, rank-one perturbation of $\mathbf{A}$.

2.  Show the **Woodbury formula** $$
     (\mathbf{A} + \mathbf{U} \mathbf{V}^T)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{I}_m + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V}^T \mathbf{A}^{-1},
    $$ where $\mathbf{A} \in \mathbb{R}^{n \times n}$ is nonsingular, $\mathbf{U}, \mathbf{V} \in \mathbb{R}^{n \times m}$, and $\mathbf{I}_m$ is the $m \times m$ identity matrix. In many applications $m$ is much smaller than $n$. Woodbury formula generalizes Sherman-Morrison and is valuable because the smaller matrix $\mathbf{I}_m + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U}$ is cheaper to invert than the larger matrix $\mathbf{A} + \mathbf{U} \mathbf{V}^T$.

3.  Show the **binomial inversion formula** $$
     (\mathbf{A} + \mathbf{U} \mathbf{B} \mathbf{V}^T)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{B}^{-1} + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V}^T \mathbf{A}^{-1},
    $$ where $\mathbf{A}$ and $\mathbf{B}$ are nonsingular.

4.  Show the identity $$
     \text{det}(\mathbf{A} + \mathbf{U} \mathbf{V}^T) = \text{det}(\mathbf{A}) \text{det}(\mathbf{I}_m + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U}).
    $$ This formula is useful for evaluating the density of a multivariate normal with covariance matrix $\mathbf{A} + \mathbf{U} \mathbf{V}^T$.

**Hint**: 1 and 2 are special cases of 3. For 4, show that $$
\begin{pmatrix}
- \mathbf{A} & \mathbf{O} \\
\mathbf{O} & \mathbf{I} + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U}
\end{pmatrix} = \begin{pmatrix}
\mathbf{I} & \mathbf{O} \\
\mathbf{V}^T \mathbf{A}^{-1} & \mathbf{I}
\end{pmatrix} \begin{pmatrix}
- \mathbf{A} & \mathbf{U} \\
\mathbf{V}^T & \mathbf{I}
\end{pmatrix} \begin{pmatrix}
\mathbf{I} & \mathbf{A}^{-1} \mathbf{U} \\
\mathbf{O} & \mathbf{I}
\end{pmatrix}.
$$

### 3.

Proof:

Firstly, we can

$$
\begin{aligned}
& \left(A+U B V^{\top}\right)\left(A^{-1}-A^{-1} U\left(B^{-1}+V^{\top} A^{-1} U\right)^{-1} V^{\top} A^{-1}\right) \\
= & I+U B V^{\top} A^{-1}-U\left(B^{-1}+V^{\top} A^{-1} U\right)^{-1} V^{\top} A^{-1}-U B V^{\top} A^{-1} U\left(B^{-1}+V^{\top} A^{-1} U\right)^{-1} V^{\top} A^{-1} \\
= & I+U B V^{\top} A^{-1}-\left(U+U B V^{\top} A^{-1} U\right)\left(B^{-1}+V^{\top} A^{-1} U\right)^{-1} V^{\top} A^{-1} \\
= & I+U B V^{\top} A^{-1}-U B\left(B^{-1}+V^{\top} A^{-1} U\right)\left(B^{-1}+V^{\top} A^{-1} U\right)^{-1} V^{\top} A^{-1} \\
= & I+U B V^{\top} A^{-1}-U B V^{\top} A^{-1} \\
= & I
\end{aligned}
$$

Since $\left(A+U B V^{\top}\right)^{-1}$ exist, we have,

$$
\begin{aligned}
\left(A+U B V^{\top}\right)^{-1} I & =(A+U B V)^{-1}\left(A+U B U^{\top}\right)\left(A^{-1}-A^{-1} U\left(B^{-1}+U^{\top} A^{-1} U\right)^{-1} U^{\top} A^{-1}\right) \\
\left(A+U B V^{\top}\right)^{-1} & \left.=A^{-1}-A^{-1} U\left(B^{-1}+V^{\top} A^{\top} U\right)^{-1} V^{\top} A^{-1}\right.
\end{aligned}
$$

### 1.

Based on what we derived in Q3, we set $\mathbf{u}=U, \mathbf{u}^{T}=V$ and $\mathbf{I}_1=B$, then we can get,

$$
    (\mathbf{A} + \mathbf{u} \mathbf{u}^T)^{-1} = \mathbf{A}^{-1} - \frac{1}{1 + \mathbf{u}^T \mathbf{A}^{-1} \mathbf{u}} \mathbf{A}^{-1} \mathbf{u} \mathbf{u}^T \mathbf{A}^{-1},
$$

where $\mathbf{A} \in \mathbb{R}^{n \times n}$ is nonsingular and $\mathbf{u} \in \mathbb{R}^n$. This formula supplies the inverse of the symmetric, rank-one perturbation of $\mathbf{A}$.

### 2.

Based on what we derived in Q3, we set $\mathbf{I}_m=B$, then we can get,

$$
    (\mathbf{A} + \mathbf{U} \mathbf{V}^T)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{I}_m + \mathbf{V}^T \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V}^T \mathbf{A}^{-1},
$$

where $\mathbf{A} \in \mathbb{R}^{n \times n}$ is nonsingular, $\mathbf{U}, \mathbf{V} \in \mathbb{R}^{n \times m}$, and $\mathbf{I}_m$ is the $m \times m$ identity matrix.

### 4.

Firstly, we can derive the relationship, $$
\begin{aligned}
& \left(\begin{array}{cc}
I & 0 \\
V^{\top} A^{-1} & I
\end{array}\right)\left(\begin{array}{cc}
-A & U \\
V^{\top} & I
\end{array}\right)\left(\begin{array}{cc}
I & A^{-1} U \\
0 & I
\end{array}\right) \\
=& \left(\begin{array}{cc}
-A& U  \\
-V^{-} A^{-1} A+V^{\top} &V^{\top} A^{-1} U+I\end{array}\right)\left(\begin{array}{cc}
I & A^{-1} U \\
0 & I
\end{array}\right) \\
=& \left(\begin{array}{cc}
-A & U \\
0 & V^{\top} A^{-1} U+I
\end{array}\right)\left(\begin{array}{cc}
I & A^{-1} U \\
0 & I
\end{array}\right) \\
=& \left(\begin{array}{cc}
-A & -A A^{-1} U+U \\
0 & V^{\top} A^{-1} U+I
\end{array}\right) \\
 =&\left(\begin{array}{cc}
-A & 0 \\
0 & I+V^{\top} A^{-1} U
\end{array}\right)
\end{aligned}
$$

Then we know

$$
\begin{aligned}
 \operatorname{det}\left(\begin{array}{cc}
-A & 0 \\
0 & I+V^{\top} A^{-1} U
\end{array}\right)&=\operatorname{det}\left(\begin{array}{cc}
I & 0 \\
V^{\top} A^{-1} & I
\end{array}\right)\left(\begin{array}{cc}
-A & U \\
V^{\top} & I
\end{array}\right)\left(\begin{array}{cc}
I & A^{-1} U \\
0 & I
\end{array}\right) \\
 \operatorname{det}(-A) \operatorname{det}\left(I+V^{\top} A^{-1} U\right)&= 1 * \operatorname{det}\left(\begin{array}{cc}
-A & U \\
V^{\top} & I
\end{array}\right)* 1 \\
 \operatorname{det}\left(A+U V^{\top}\right)&=\operatorname{det}(A) \operatorname{det}\left(I+V^{\top} A^{-1} U\right)
\end{aligned}
$$

## Q6

Demonstrate the following facts about triangular and orthogonal matrices in Julia (one numerical example for each fact). Mathematically curious ones are also encouraged to prove them.

Note a unit triangular matrix is a triangular matrix with all diagonal entries being 1.

1.  The product of two upper (lower) triangular matrices is upper (lower) triangular.

2.  The inverse of an upper (lower) triangular matrix is upper (lower) triangular.

3.  The product of two unit upper (lower) triangular matrices is unit upper (lower) triangular.

4.  The inverse of a unit upper (lower) triangular matrix is unit upper (lower) triangular.

5.  An orthogonal upper (lower) triangular matrix is diagonal. (You just need to prove this.)

6.  The product of two orthogonal matrices is orthogonal.

### 1.

```{julia}
Random.seed!(257)
upp1 = UpperTriangular(randn(3, 3))
upp2 = UpperTriangular(randn(3, 3))
upp1 * upp2
```

### 2.

```{julia}
Random.seed!(257)
upp = UpperTriangular(randn(3, 3))
inv(upp)
```

### 3.

```{julia}
Random.seed!(257)
upp1 = UpperTriangular(randn(3, 3))
upp2 = UpperTriangular(randn(3, 3))
for i in 1:3
  upp1[i, i] = 1
  upp2[i, i] = 1
end
upp1 * upp2 
```

### 4.

```{julia}
Random.seed!(257)
upp = UpperTriangular(randn(3, 3))
for i in 1:3
  upp[i, i] = 1
end
inv(upp)
```

### 5.

Consider a $2 \times 2$ orthogonal upper matrix $A$,

$$
\text { Let } A=\left[\begin{array}{ll}
a & b \\
0 & c
\end{array}\right]
$$

Since it is orthogonal, we know $AA^{'}=I$,

$$
\begin{aligned}
\Rightarrow & {\left[\begin{array}{ll}
a & b \\
0 & c
\end{array}\right]\left[\begin{array}{ll}
a & 0 \\
b & c
\end{array}\right]=\left[\begin{array}{cc}
a^{2}+b^{2} & b c \\
c b & c^{2}
\end{array}\right]=I }
\end{aligned}
$$

Then it's easy to get,

$$
\left\{\begin{array}{l}
a= \pm 1 \\
b=0 \\
c= \pm 1
\end{array}\right.
$$

in $3 \times 3$ matrix $A$, we can partition it into 4 parts,

$$
A=\left[\begin{array}{ll}B & C \\ 0 & D\end{array}\right]
$$ 
, here $B$ is $2 \times 2$ upper matrix.

Similar as $2 \times 2$ case, we can get 

$$
\left\{\begin{array}{l}
B^{'}B=I_{2} \\
C=0 \\
D= \pm 1
\end{array}\right.
$$

Since we already know $2 \times 2$ orthogonal upper matrix is diagonal, then we know $B$ is diagonal.

Then, by induction, we know that all orthogonal upper (lower) triangular matrix is diagonal.

### 6.

```{julia}
Random.seed!(257)
Q1, R1 = qr(randn(3,3))
Q2, R2 = qr(randn(3,3))
round.(transpose(Q1 * Q2) * (Q1 * Q2); digits = 4)
```


Let $A, B$ be two compatible orthogonal matrix, we have,

$$
A^{'} A=I\\
B^{'} B=I
$$

Then, 

$$
\begin{aligned}
(A B)^{\prime}(A B)&=B^{\prime} A^{\prime} A B\\
& =B^{\prime} I B \\
& =I
\end{aligned}
$$

Thus we know $AB$ is also orthogonal matrix

## Q7

Let the $n \times n$ matrix `H` have elements `H[i, j] = 1 / (i + j - 1)`.

1.  Write a function `h(n)` that outputs $n \times n$ matrix `H`. Try at least 4 ways, e.g., $ij$-looping, $ji$-looping, [comprehension](https://docs.julialang.org/en/v1/manual/arrays/#man-comprehensions), and vectorization (R style). Compute and print `H` for `n = 5`.

2.  Compare their efficiencies (using `BenchmarkTools`) at `n = 5000`.

### 1.

#### $ij$-looping

```{julia}
n = 5
H = Matrix{Float64}(undef, n, n)
function hij(n::Int, H::Matrix{Float64})
    for i in 1:n, j in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hij(5, H)
```

#### $ji$-looping

```{julia}
function hji(n::Int, H::Matrix{Float64})
    for j in 1:n, i in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hji(5, H)
```

#### $ij$-looping (with turbo speed up)

```{julia}
function hijt(n::Int, H::Matrix{Float64})
    @turbo for i in 1:n, j in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hijt(5, H)
```

#### $ji$-looping (with turbo speed up)

```{julia}
function hjit(n::Int, H::Matrix{Float64})
    @turbo for j in 1:n, i in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hjit(5, H)
```

#### $ij$-looping (with tturbo speed up)

```{julia}
function hijtt(n::Int, H::Matrix{Float64})
    @tturbo for i in 1:n, j in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hijtt(5, H)
```

#### $ji$-looping (with tturbo speed up)

```{julia}
function hjitt(n::Int, H::Matrix{Float64})
    @tturbo for j in 1:n, i in 1:n
        H[i, j] = 1 / (i + j - 1)
    end
    H
end
hjitt(5, H)
```

#### comprehension

```{julia}
function hcom(n::Int, H::Matrix{Float64})
    H = [1 / (i + j - 1) for i in 1:n, j in 1:n]
    H
end
hcom(5, H)
```

#### vectorization

```{julia}
function hvec(indices::UnitRange{Int64}, H::Matrix{Float64})
    H = 1 ./ (indices .+ transpose(indices) .- 1)
    H
end
hvec(1:5, H)
```

### 2.

```{julia}
# prepare matrix and dictionary
n = 5000
H = Matrix{Float64}(undef, n, n)
benchmark_result = Dict() 
```

```{julia}
# ji looping
bmij = @benchmark hij(5000, $H)
```

```{julia}
# ji looping
bmji = @benchmark hji(5000, $H)
```

```{julia}
# ij looping (with turbo speed up)
bmijt = @benchmark hijt(5000, $H)
```

```{julia}
# ji looping (with turbo speed up)
bmjit = @benchmark hjit(5000, $H)
```

```{julia}
# ij looping (with tturbo speed up)
bmijtt = @benchmark hijtt(5000, $H)
```

```{julia}
# ji looping (with tturbo speed up)
bmjitt = @benchmark hjitt(5000, $H)
```

```{julia}
# comprehension
bmcom = @benchmark hcom(5000, $H)
```

```{julia}
# vectorization
bmvec = @benchmark hvec(1:5000, $H)
```

#### Summary table

```{julia}
benchmark_result["ij-looping"] = median(bmij.times) 
benchmark_result["ji-looping"] = median(bmji.times) 
benchmark_result["ij-looping (turbo)"] = median(bmijt.times) 
benchmark_result["ji-looping (turbo)"] = median(bmjit.times) 
benchmark_result["ij-looping (tturbo)"] = median(bmijtt.times) 
benchmark_result["ji-looping (tturbo)"] = median(bmjitt.times) 
benchmark_result["comprehension"] = median(bmcom.times) 
benchmark_result["vectorization"] = median(bmvec.times) 
sort(collect(benchmark_result), by = x -> x[2])
```

As we can tell from the table, `ij-looping (turbo)` performed the best. When equipped with `turbo` tool, both `ij-looping` and `ji-looping` performs better than `comprehension` and `vectorization`. However, without `turbo` tool, `ij-looping` is the slower than `comprehension` and `vectorization`. Here we can also see that the multithreads `tturbo` did not improve the speed. The possible reason is that the number of the loop did is not large enough and it also took time to run the multithreads function.

